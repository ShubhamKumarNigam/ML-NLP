## Load the Model

Inspired from [BERT Word Embeddings Tutorial](http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/) by Chris McCormick.


Install the huggingface implementation.


```python
!pip install pytorch-pretrained-bert
```

    Collecting pytorch-pretrained-bert
    [?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 2.8MB/s 
    [?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.6.0+cu101)
    Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.5)
    Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)
    Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.47)
    Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)
    Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)
    Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)
    Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)
    Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)
    Requirement already satisfied: botocore<1.18.0,>=1.17.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.17.47)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.10)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.6.20)
    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)
    Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.47->boto3->pytorch-pretrained-bert) (2.8.1)
    Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.47->boto3->pytorch-pretrained-bert) (0.15.2)
    Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.47->boto3->pytorch-pretrained-bert) (1.15.0)
    Installing collected packages: pytorch-pretrained-bert
    Successfully installed pytorch-pretrained-bert-0.6.2



```python
import torch
from pytorch_pretrained_bert import BertTokenizer

# Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231508/231508 [00:00<00:00, 422405.20B/s]


## Inspect BERT Vocabulary
--------------------------

### Vocab Dump
--------------

Retrieve the entire list of "tokens" and write these out to text files so we can peruse them.


```python
with open("vocabulary.txt", 'w') as f:
    
    # For each token...
    for token in tokenizer.vocab.keys():
        
        # Write it out and escape any unicode characters.            
        f.write(token + '\n')

```

From perusing the vocab, I'm seeing that:

* The first 999 tokens (1-indexed) appear to be reserved, and most are of the form [unused957].
    * 1   - [PAD]
    * 101 - [UNK]
    * 102 - [CLS]
    * 103 - [SEP]
    * 104 - [MASK]
* Rows 1000-1996 appear to be a dump of individual characters. 
    * They don't appear to be sorted by frequency (e.g., the letters of the alphabet are all in sequence).
* The first word is "the" at position 1997.
    * From there, the words appear to be sorted by frequency. 
    * The top ~18 words are whole words, and then number 2016 is ##s, presumably the most common subword.
    * The last whole word is at 29612, "necessitated"

Some funny inclusions:
* starbucks
* triassic
* abolitionist
* 1679

### Single Characters
---------------------

The following code prints out all of the single character tokens in vocabulary, as well as all of the single-character tokens preceded by '##'.

It turns out that these are matching sets--for every standalone character there is also a '##' version. There are 997 single character tokens.

The following cell iterates over the vocabulary, pulling out all of the single character tokens.


```python
len(tokenizer.vocab)
```




    30522




```python
one_chars = []
one_chars_hashes = []

# For each token in the vocabulary...
for token in tokenizer.vocab.keys():
    
    # Record any single-character tokens.
    if len(token) == 1:
        one_chars.append(token)
    
    # Record single-character tokens preceded by the two hashes.    
    elif len(token) == 3 and token[0:2] == '##':
        one_chars_hashes.append(token)

```


```python
print('Number of single character tokens:', len(one_chars), '\n')

# Print all of the single characters, 40 per row.

# For every batch of 40 tokens...
for i in range(0, len(one_chars), 40):
    
    # Limit the end index so we don't go past the end of the list.
    end = min(i + 40, len(one_chars) + 1)
    
    # Print out the tokens, separated by a space.
    print(' '.join(one_chars[i:end]))
```

    Number of single character tokens: 997 
    
    ! " # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ [ \ ] ^ _ ` a b
    c d e f g h i j k l m n o p q r s t u v w x y z { | } ~ Â¡ Â¢ Â£ Â¤ Â¥ Â¦ Â§ Â¨ Â© Âª Â« Â¬
    Â® Â° Â± Â² Â³ Â´ Âµ Â¶ Â· Â¹ Âº Â» Â¼ Â½ Â¾ Â¿ Ã— ÃŸ Ã¦ Ã° Ã· Ã¸ Ã¾ Ä‘ Ä§ Ä± Å‚ Å‹ Å“ Æ’ É É‘ É’ É” É• É™ É› É¡ É£ É¨
    Éª É« É¬ É¯ É² É´ É¹ É¾ Ê€ Ê Ê‚ Êƒ Ê‰ ÊŠ Ê‹ ÊŒ Ê Ê Ê‘ Ê’ Ê” Ê° Ê² Ê³ Ê· Ê¸ Ê» Ê¼ Ê¾ Ê¿ Ëˆ Ë Ë¡ Ë¢ Ë£ Ë¤ Î± Î² Î³ Î´
    Îµ Î¶ Î· Î¸ Î¹ Îº Î» Î¼ Î½ Î¾ Î¿ Ï€ Ï Ï‚ Ïƒ Ï„ Ï… Ï† Ï‡ Ïˆ Ï‰ Ğ° Ğ± Ğ² Ğ³ Ğ´ Ğµ Ğ¶ Ğ· Ğ¸ Ğº Ğ» Ğ¼ Ğ½ Ğ¾ Ğ¿ Ñ€ Ñ Ñ‚ Ñƒ
    Ñ„ Ñ… Ñ† Ñ‡ Ñˆ Ñ‰ ÑŠ Ñ‹ ÑŒ Ñ Ñ Ñ Ñ’ Ñ” Ñ– Ñ˜ Ñ™ Ñš Ñ› Ó Õ¡ Õ¢ Õ£ Õ¤ Õ¥ Õ© Õ« Õ¬ Õ¯ Õ° Õ´ Õµ Õ¶ Õ¸ Õº Õ½ Õ¾ Õ¿ Ö€ Ö‚
    Ö„ Ö¾ × ×‘ ×’ ×“ ×” ×• ×– ×— ×˜ ×™ ×š ×› ×œ × × ×Ÿ ×  ×¡ ×¢ ×£ ×¤ ×¥ ×¦ ×§ ×¨ ×© ×ª ØŒ Ø¡ Ø§ Ø¨ Ø© Øª Ø« Ø¬ Ø­ Ø® Ø¯
    Ø° Ø± Ø² Ø³ Ø´ Øµ Ø¶ Ø· Ø¸ Ø¹ Øº Ù€ Ù Ù‚ Ùƒ Ù„ Ù… Ù† Ù‡ Ùˆ Ù‰ ÙŠ Ù¹ Ù¾ Ú† Ú© Ú¯ Úº Ú¾ Û ÛŒ Û’ à¤… à¤† à¤‰ à¤ à¤• à¤– à¤— à¤š
    à¤œ à¤Ÿ à¤¡ à¤£ à¤¤ à¤¥ à¤¦ à¤§ à¤¨ à¤ª à¤¬ à¤­ à¤® à¤¯ à¤° à¤² à¤µ à¤¶ à¤· à¤¸ à¤¹ à¤¾ à¤¿ à¥€ à¥‹ à¥¤ à¥¥ à¦‚ à¦… à¦† à¦‡ à¦‰ à¦ à¦“ à¦• à¦– à¦— à¦š à¦› à¦œ
    à¦Ÿ à¦¡ à¦£ à¦¤ à¦¥ à¦¦ à¦§ à¦¨ à¦ª à¦¬ à¦­ à¦® à¦¯ à¦° à¦² à¦¶ à¦· à¦¸ à¦¹ à¦¾ à¦¿ à§€ à§‡ à®• à®š à®Ÿ à®¤ à®¨ à®© à®ª à®® à®¯ à®° à®² à®³ à®µ à®¾ à®¿ à¯ à¯‡
    à¯ˆ à²¨ à²° à²¾ à¶š à¶º à¶» à¶½ à·€ à· à¸ à¸‡ à¸• à¸— à¸™ à¸ à¸¡ à¸¢ à¸£ à¸¥ à¸§ à¸ª à¸­ à¸² à¹€ à¼‹ à¼ à½‚ à½„ à½‘ à½“ à½” à½– à½˜ à½  à½¢ à½£ à½¦ á€™ áƒ
    áƒ‘ áƒ’ áƒ“ áƒ” áƒ• áƒ— áƒ˜ áƒ™ áƒš áƒ› áƒœ áƒ áƒ  áƒ¡ áƒ¢ áƒ£ á„€ á„‚ á„ƒ á„… á„† á„‡ á„‰ á„Š á„‹ á„Œ á„ á„ á„ á„‘ á„’ á…¡ á…¢ á…¥ á…¦ á…§ á…© á…ª á…­ á…®
    á…¯ á…² á…³ á…´ á…µ á†¨ á†« á†¯ á†· á†¸ á†¼ á´¬ á´® á´° á´µ á´º áµ€ áµƒ áµ‡ áµˆ áµ‰ áµ áµ áµ áµ’ áµ– áµ— áµ˜ áµ¢ áµ£ áµ¤ áµ¥ á¶œ á¶  â€ â€‘ â€’ â€“ â€” â€•
    â€– â€˜ â€™ â€š â€œ â€ â€ â€  â€¡ â€¢ â€¦ â€° â€² â€³ â€º â€¿ â„ â° â± â´ âµ â¶ â· â¸ â¹ âº â» â¿ â‚€ â‚ â‚‚ â‚ƒ â‚„ â‚… â‚† â‚‡ â‚ˆ â‚‰ â‚Š â‚
    â‚ â‚ â‚‘ â‚’ â‚“ â‚• â‚– â‚— â‚˜ â‚™ â‚š â‚› â‚œ â‚¤ â‚© â‚¬ â‚± â‚¹ â„“ â„– â„ â„¢ â…“ â…” â† â†‘ â†’ â†“ â†” â†¦ â‡„ â‡Œ â‡’ âˆ‚ âˆ… âˆ† âˆ‡ âˆˆ âˆ’ âˆ—
    âˆ˜ âˆš âˆ âˆ§ âˆ¨ âˆ© âˆª â‰ˆ â‰¡ â‰¤ â‰¥ âŠ‚ âŠ† âŠ• âŠ— â‹… â”€ â”‚ â–  â–ª â— â˜… â˜† â˜‰ â™  â™£ â™¥ â™¦ â™­ â™¯ âŸ¨ âŸ© â±¼ âº© âº¼ â½¥ ã€ ã€‚ ã€ˆ ã€‰
    ã€Š ã€‹ ã€Œ ã€ ã€ ã€ ã€œ ã‚ ã„ ã† ãˆ ãŠ ã‹ ã ã ã‘ ã“ ã• ã— ã™ ã› ã ãŸ ã¡ ã£ ã¤ ã¦ ã¨ ãª ã« ã¬ ã­ ã® ã¯ ã² ãµ ã¸ ã» ã¾ ã¿
    ã‚€ ã‚ ã‚‚ ã‚„ ã‚† ã‚ˆ ã‚‰ ã‚Š ã‚‹ ã‚Œ ã‚ ã‚’ ã‚“ ã‚¡ ã‚¢ ã‚£ ã‚¤ ã‚¦ ã‚§ ã‚¨ ã‚ª ã‚« ã‚­ ã‚¯ ã‚± ã‚³ ã‚µ ã‚· ã‚¹ ã‚» ã‚¿ ãƒ ãƒƒ ãƒ„ ãƒ† ãƒˆ ãƒŠ ãƒ‹ ãƒ ãƒ
    ãƒ’ ãƒ• ãƒ˜ ãƒ› ãƒ ãƒŸ ãƒ  ãƒ¡ ãƒ¢ ãƒ£ ãƒ¥ ãƒ§ ãƒ© ãƒª ãƒ« ãƒ¬ ãƒ­ ãƒ¯ ãƒ³ ãƒ» ãƒ¼ ä¸€ ä¸‰ ä¸Š ä¸‹ ä¸ ä¸– ä¸­ ä¸» ä¹… ä¹‹ ä¹Ÿ äº‹ äºŒ äº” äº• äº¬ äºº äº» ä»
    ä»‹ ä»£ ä»® ä¼Š ä¼š ä½ ä¾ ä¿ ä¿¡ å¥ å…ƒ å…‰ å…« å…¬ å†… å‡º åˆ† å‰ åŠ‰ åŠ› åŠ  å‹ åŒ— åŒº å åƒ å— åš åŸ å£ å¤ å² å¸ åˆ å‰ åŒ å å’Œ å›— å››
    å›½ åœ‹ åœŸ åœ° å‚ åŸ å ‚ å ´ å£« å¤ å¤– å¤§ å¤© å¤ª å¤« å¥ˆ å¥³ å­ å­¦ å®€ å®‡ å®‰ å®— å®š å®£ å®® å®¶ å®¿ å¯º å°‡ å° å°š å±± å²¡ å³¶ å´ å· å· å·¿ å¸
    å¹³ å¹´ å¹¸ å¹¿ å¼˜ å¼µ å½³ å¾Œ å¾¡ å¾· å¿ƒ å¿„ å¿— å¿  æ„› æˆ æˆ‘ æˆ¦ æˆ¸ æ‰‹ æ‰Œ æ”¿ æ–‡ æ–° æ–¹ æ—¥ æ˜ æ˜Ÿ æ˜¥ æ˜­ æ™º æ›² æ›¸ æœˆ æœ‰ æœ æœ¨ æœ¬ æ æ‘
    æ± æ¾ æ— æ£® æ¥Š æ¨¹ æ©‹ æ­Œ æ­¢ æ­£ æ­¦ æ¯” æ° æ°‘ æ°´ æ°µ æ°· æ°¸ æ±Ÿ æ²¢ æ²³ æ²» æ³• æµ· æ¸… æ¼¢ ç€¬ ç« ç‰ˆ çŠ¬ ç‹ ç”Ÿ ç”° ç”· ç–’ ç™º ç™½ çš„ çš‡ ç›®
    ç›¸ çœ çœŸ çŸ³ ç¤º ç¤¾ ç¥ ç¦ ç¦¾ ç§€ ç§‹ ç©º ç«‹ ç«  ç«¹ ç³¹ ç¾ ç¾© è€³ è‰¯ è‰¹ èŠ± è‹± è¯ è‘‰ è—¤ è¡Œ è¡— è¥¿ è¦‹ è¨ èª è°· è² è²´ è»Š è» è¾¶ é“ éƒ
    éƒ¡ éƒ¨ éƒ½ é‡Œ é‡ é‡‘ éˆ´ é•‡ é•· é–€ é–“ é˜ é˜¿ é™³ é™½ é›„ é’ é¢ é¢¨ é£Ÿ é¦™ é¦¬ é«˜ é¾ é¾¸ ï¬ ï¬‚ ï¼ ï¼ˆ ï¼‰ ï¼Œ ï¼ ï¼ ï¼ ï¼š ï¼Ÿ ï½



```python
print('Number of single character tokens with hashes:', len(one_chars_hashes), '\n')

# Print all of the single characters, 40 per row.

# Strip the hash marks, since they just clutter the display.
tokens = [token.replace('##', '') for token in one_chars_hashes]

# For every batch of 40 tokens...
for i in range(0, len(tokens), 40):
    
    # Limit the end index so we don't go past the end of the list.
    end = min(i + 40, len(tokens) + 1)
    
    # Print out the tokens, separated by a space.
    print(' '.join(tokens[i:end]))
```

    Number of single character tokens with hashes: 997 
    
    s a e i n o d r y t l m u h k c g p 2 z 1 b 3 f 4 6 7 x v 8 5 9 0 w j q Â° â‚‚ Ğ° Ğ¸
    Â² â‚ƒ Ä± â‚ âº Â½ Ğ¾ Ù‡ ÙŠ Î± Ğµ Ø¯ Ù† Î½ Ã¸ Ñ€ â‚„ â‚€ Ø± Ñ Â³ Î¹ Å‚ Ğ½ áµ¢ â‚™ ÃŸ Ø© Ï‚ Ù… âˆ’ Ñ‚ Ë Ù„ ÑŒ Ğº â™­ Î· ÛŒ Ğ²
    Ø§ Ã— Â¹ Ñ‹ ×” É› Ğ» ! " # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \ ] ^ _ ` { | } ~ Â¡
    Â¢ Â£ Â¤ Â¥ Â¦ Â§ Â¨ Â© Âª Â« Â¬ Â® Â± Â´ Âµ Â¶ Â· Âº Â» Â¼ Â¾ Â¿ Ã¦ Ã° Ã· Ã¾ Ä‘ Ä§ Å‹ Å“ Æ’ É É‘ É’ É” É• É™ É¡ É£ É¨
    Éª É« É¬ É¯ É² É´ É¹ É¾ Ê€ Ê Ê‚ Êƒ Ê‰ ÊŠ Ê‹ ÊŒ Ê Ê Ê‘ Ê’ Ê” Ê° Ê² Ê³ Ê· Ê¸ Ê» Ê¼ Ê¾ Ê¿ Ëˆ Ë¡ Ë¢ Ë£ Ë¤ Î² Î³ Î´ Îµ Î¶
    Î¸ Îº Î» Î¼ Î¾ Î¿ Ï€ Ï Ïƒ Ï„ Ï… Ï† Ï‡ Ïˆ Ï‰ Ğ± Ğ³ Ğ´ Ğ¶ Ğ· Ğ¼ Ğ¿ Ñ Ñƒ Ñ„ Ñ… Ñ† Ñ‡ Ñˆ Ñ‰ ÑŠ Ñ Ñ Ñ’ Ñ” Ñ– Ñ˜ Ñ™ Ñš Ñ›
    Ó Õ¡ Õ¢ Õ£ Õ¤ Õ¥ Õ© Õ« Õ¬ Õ¯ Õ° Õ´ Õµ Õ¶ Õ¸ Õº Õ½ Õ¾ Õ¿ Ö€ Ö‚ Ö„ Ö¾ × ×‘ ×’ ×“ ×• ×– ×— ×˜ ×™ ×š ×› ×œ × × ×Ÿ ×  ×¡
    ×¢ ×£ ×¤ ×¥ ×¦ ×§ ×¨ ×© ×ª ØŒ Ø¡ Ø¨ Øª Ø« Ø¬ Ø­ Ø® Ø° Ø² Ø³ Ø´ Øµ Ø¶ Ø· Ø¸ Ø¹ Øº Ù€ Ù Ù‚ Ùƒ Ùˆ Ù‰ Ù¹ Ù¾ Ú† Ú© Ú¯ Úº Ú¾
    Û Û’ à¤… à¤† à¤‰ à¤ à¤• à¤– à¤— à¤š à¤œ à¤Ÿ à¤¡ à¤£ à¤¤ à¤¥ à¤¦ à¤§ à¤¨ à¤ª à¤¬ à¤­ à¤® à¤¯ à¤° à¤² à¤µ à¤¶ à¤· à¤¸ à¤¹ à¤¾ à¤¿ à¥€ à¥‹ à¥¤ à¥¥ à¦‚ à¦… à¦†
    à¦‡ à¦‰ à¦ à¦“ à¦• à¦– à¦— à¦š à¦› à¦œ à¦Ÿ à¦¡ à¦£ à¦¤ à¦¥ à¦¦ à¦§ à¦¨ à¦ª à¦¬ à¦­ à¦® à¦¯ à¦° à¦² à¦¶ à¦· à¦¸ à¦¹ à¦¾ à¦¿ à§€ à§‡ à®• à®š à®Ÿ à®¤ à®¨ à®© à®ª
    à®® à®¯ à®° à®² à®³ à®µ à®¾ à®¿ à¯ à¯‡ à¯ˆ à²¨ à²° à²¾ à¶š à¶º à¶» à¶½ à·€ à· à¸ à¸‡ à¸• à¸— à¸™ à¸ à¸¡ à¸¢ à¸£ à¸¥ à¸§ à¸ª à¸­ à¸² à¹€ à¼‹ à¼ à½‚ à½„ à½‘
    à½“ à½” à½– à½˜ à½  à½¢ à½£ à½¦ á€™ áƒ áƒ‘ áƒ’ áƒ“ áƒ” áƒ• áƒ— áƒ˜ áƒ™ áƒš áƒ› áƒœ áƒ áƒ  áƒ¡ áƒ¢ áƒ£ á„€ á„‚ á„ƒ á„… á„† á„‡ á„‰ á„Š á„‹ á„Œ á„ á„ á„ á„‘
    á„’ á…¡ á…¢ á…¥ á…¦ á…§ á…© á…ª á…­ á…® á…¯ á…² á…³ á…´ á…µ á†¨ á†« á†¯ á†· á†¸ á†¼ á´¬ á´® á´° á´µ á´º áµ€ áµƒ áµ‡ áµˆ áµ‰ áµ áµ áµ áµ’ áµ– áµ— áµ˜ áµ£ áµ¤
    áµ¥ á¶œ á¶  â€ â€‘ â€’ â€“ â€” â€• â€– â€˜ â€™ â€š â€œ â€ â€ â€  â€¡ â€¢ â€¦ â€° â€² â€³ â€º â€¿ â„ â° â± â´ âµ â¶ â· â¸ â¹ â» â¿ â‚… â‚† â‚‡ â‚ˆ
    â‚‰ â‚Š â‚ â‚ â‚ â‚‘ â‚’ â‚“ â‚• â‚– â‚— â‚˜ â‚š â‚› â‚œ â‚¤ â‚© â‚¬ â‚± â‚¹ â„“ â„– â„ â„¢ â…“ â…” â† â†‘ â†’ â†“ â†” â†¦ â‡„ â‡Œ â‡’ âˆ‚ âˆ… âˆ† âˆ‡ âˆˆ
    âˆ— âˆ˜ âˆš âˆ âˆ§ âˆ¨ âˆ© âˆª â‰ˆ â‰¡ â‰¤ â‰¥ âŠ‚ âŠ† âŠ• âŠ— â‹… â”€ â”‚ â–  â–ª â— â˜… â˜† â˜‰ â™  â™£ â™¥ â™¦ â™¯ âŸ¨ âŸ© â±¼ âº© âº¼ â½¥ ã€ ã€‚ ã€ˆ ã€‰
    ã€Š ã€‹ ã€Œ ã€ ã€ ã€ ã€œ ã‚ ã„ ã† ãˆ ãŠ ã‹ ã ã ã‘ ã“ ã• ã— ã™ ã› ã ãŸ ã¡ ã£ ã¤ ã¦ ã¨ ãª ã« ã¬ ã­ ã® ã¯ ã² ãµ ã¸ ã» ã¾ ã¿
    ã‚€ ã‚ ã‚‚ ã‚„ ã‚† ã‚ˆ ã‚‰ ã‚Š ã‚‹ ã‚Œ ã‚ ã‚’ ã‚“ ã‚¡ ã‚¢ ã‚£ ã‚¤ ã‚¦ ã‚§ ã‚¨ ã‚ª ã‚« ã‚­ ã‚¯ ã‚± ã‚³ ã‚µ ã‚· ã‚¹ ã‚» ã‚¿ ãƒ ãƒƒ ãƒ„ ãƒ† ãƒˆ ãƒŠ ãƒ‹ ãƒ ãƒ
    ãƒ’ ãƒ• ãƒ˜ ãƒ› ãƒ ãƒŸ ãƒ  ãƒ¡ ãƒ¢ ãƒ£ ãƒ¥ ãƒ§ ãƒ© ãƒª ãƒ« ãƒ¬ ãƒ­ ãƒ¯ ãƒ³ ãƒ» ãƒ¼ ä¸€ ä¸‰ ä¸Š ä¸‹ ä¸ ä¸– ä¸­ ä¸» ä¹… ä¹‹ ä¹Ÿ äº‹ äºŒ äº” äº• äº¬ äºº äº» ä»
    ä»‹ ä»£ ä»® ä¼Š ä¼š ä½ ä¾ ä¿ ä¿¡ å¥ å…ƒ å…‰ å…« å…¬ å†… å‡º åˆ† å‰ åŠ‰ åŠ› åŠ  å‹ åŒ— åŒº å åƒ å— åš åŸ å£ å¤ å² å¸ åˆ å‰ åŒ å å’Œ å›— å››
    å›½ åœ‹ åœŸ åœ° å‚ åŸ å ‚ å ´ å£« å¤ å¤– å¤§ å¤© å¤ª å¤« å¥ˆ å¥³ å­ å­¦ å®€ å®‡ å®‰ å®— å®š å®£ å®® å®¶ å®¿ å¯º å°‡ å° å°š å±± å²¡ å³¶ å´ å· å· å·¿ å¸
    å¹³ å¹´ å¹¸ å¹¿ å¼˜ å¼µ å½³ å¾Œ å¾¡ å¾· å¿ƒ å¿„ å¿— å¿  æ„› æˆ æˆ‘ æˆ¦ æˆ¸ æ‰‹ æ‰Œ æ”¿ æ–‡ æ–° æ–¹ æ—¥ æ˜ æ˜Ÿ æ˜¥ æ˜­ æ™º æ›² æ›¸ æœˆ æœ‰ æœ æœ¨ æœ¬ æ æ‘
    æ± æ¾ æ— æ£® æ¥Š æ¨¹ æ©‹ æ­Œ æ­¢ æ­£ æ­¦ æ¯” æ° æ°‘ æ°´ æ°µ æ°· æ°¸ æ±Ÿ æ²¢ æ²³ æ²» æ³• æµ· æ¸… æ¼¢ ç€¬ ç« ç‰ˆ çŠ¬ ç‹ ç”Ÿ ç”° ç”· ç–’ ç™º ç™½ çš„ çš‡ ç›®
    ç›¸ çœ çœŸ çŸ³ ç¤º ç¤¾ ç¥ ç¦ ç¦¾ ç§€ ç§‹ ç©º ç«‹ ç«  ç«¹ ç³¹ ç¾ ç¾© è€³ è‰¯ è‰¹ èŠ± è‹± è¯ è‘‰ è—¤ è¡Œ è¡— è¥¿ è¦‹ è¨ èª è°· è² è²´ è»Š è» è¾¶ é“ éƒ
    éƒ¡ éƒ¨ éƒ½ é‡Œ é‡ é‡‘ éˆ´ é•‡ é•· é–€ é–“ é˜ é˜¿ é™³ é™½ é›„ é’ é¢ é¢¨ é£Ÿ é¦™ é¦¬ é«˜ é¾ é¾¸ ï¬ ï¬‚ ï¼ ï¼ˆ ï¼‰ ï¼Œ ï¼ ï¼ ï¼ ï¼š ï¼Ÿ ï½



```python
print('Are the two sets identical?', set(one_chars) == set(tokens))
```

    Are the two sets identical? True


### Subwords vs. Whole-words
Let's gather some statistics on the vocabulary.




```python
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

sns.set(style='darkgrid')

# Increase the plot size and font size.
sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (10,5)

# Measure the length of every token in the vocab.
token_lengths = [len(token) for token in tokenizer.vocab.keys()]

# Plot the number of tokens of each length.
sns.countplot(token_lengths)
plt.title('Vocab Token Lengths')
plt.xlabel('Token Length')
plt.ylabel('# of Tokens')

print('Maximum token length:', max(token_lengths), len(token_lengths))
```

    Maximum token length: 18 30522



![png](output_17_1.png)


Let's look at just the tokens which begin with '##'.


```python
num_subwords = 0

subword_lengths = []

# For each token in the vocabulary...
for token in tokenizer.vocab.keys():
    
    # If it's a subword...
    if len(token) >= 2 and token[0:2] == '##':
        
        # Tally all subwords
        num_subwords += 1

        # Measure the sub word length (without the hashes)
        length = len(token) - 2

        # Record the lengths.        
        subword_lengths.append(length)

```

How many '##' tokens are there vs. the full vocab?


```python
vocab_size = len(tokenizer.vocab.keys())

print('Number of subwords: {:,} of {:,}'.format(num_subwords, vocab_size))

# Calculate the percentage of words that are '##' subwords.
prcnt = float(num_subwords) / vocab_size * 100.0

print('%.2f%%' % prcnt)
```

    Number of subwords: 5,828 of 30,522
    19.09%


Plot the subword lengths (not including the two '##' characters).


```python
for token in tokenizer.vocab.keys():
  if len(token) == 12 and token[0:2] == "##":
    print(token)


sns.countplot(subword_lengths)
plt.title('Subword Token Lengths (w/o "##")')
plt.xlabel('Subword Length')
plt.ylabel('# of ## Subwords')
```

    ##orestation
    ##ropriation
    ##filtration





    Text(0, 0.5, '# of ## Subwords')




![png](output_23_2.png)


### Misspellings


```python
'misspelled' in tokenizer.vocab # Right
```




    False




```python
'mispelled' in tokenizer.vocab # Wrong
```




    False




```python
'government' in tokenizer.vocab # Right
```




    True




```python
'goverment' in tokenizer.vocab # Wrong
```




    False




```python
'beginning' in tokenizer.vocab # Right
```




    True




```python
'begining' in tokenizer.vocab # Wrong
```




    False




```python
'separate' in tokenizer.vocab # Right
```




    True




```python
'seperate' in tokenizer.vocab # Wrong
```




    False



What about contractions?


```python
"can't" in tokenizer.vocab
```




    False




```python
"cant" in tokenizer.vocab
```




    False




```python
"don't" in tokenizer.vocab
```




    False



### Start vs. Mid Subwords

For single characters, there are both the individual character and the '##' version for every character. Is the same true of subwords?


```python
# For each token in the vocabulary...
for token in tokenizer.vocab.keys():
    
    # If it's a subword...
    if len(token) >= 2 and token[0:2] == '##':
        if not token[2:] in tokenizer.vocab:
            print('Did not find a token for', token[2:])
            break
```

    Did not find a token for ly



```python
'##ly' in tokenizer.vocab
```




    True




```python
'ly' in tokenizer.vocab
```




    False



### Names




```python
!pip install wget
```

    Collecting wget
      Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip
    Building wheels for collected packages: wget
      Building wheel for wget (setup.py) ... [?25l[?25hdone
      Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=79cf5d66f6b49bde1be79b4d8cdb7a769840d85270fb59441106c2a8cafdb50f
      Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f
    Successfully built wget
    Installing collected packages: wget
    Successfully installed wget-3.2



```python
import wget
import random 

print('Beginning file download with wget module')

url = 'http://www.gutenberg.org/files/3201/files/NAMES.TXT'
wget.download(url, 'first-names.txt')

```

    Beginning file download with wget module





    'first-names.txt'




```python
# Read them in.
with open('first-names.txt', 'rb') as f:
    names_encoded = f.readlines()

names = []

# Decode the names, convert to lowercase, and strip newlines.
for name in names_encoded:
    try:
        names.append(name.rstrip().lower().decode('utf-8'))
    except:
        continue

print('Number of names: {:,}'.format(len(names)))
print('Example:', random.choice(names))

```

    Number of names: 21,985
    Example: ethelbert



```python
print(names_encoded[:5])
print(names[:5]) # so decoding is important here
```

    [b'Aaberg\r\n', b'Aalst\r\n', b'Aara\r\n', b'Aaren\r\n', b'Aarika\r\n']
    ['aaberg', 'aalst', 'aara', 'aaren', 'aarika']



```python
num_names = 0

# For each name in our list...
for name in names:

    # If it's in the vocab...
    if name in tokenizer.vocab:
        # Tally it.
        num_names += 1

print('{:,} names in the vocabulary'.format(num_names))
```

    3,869 names in the vocabulary



```python
"lee" in names
```




    True



**Further Research**
* Add more modern names
    * This repo / file contains some more modern names. The file download isn't working, though.
    * `https://raw.githubusercontent.com/arineng/arincli/master/lib/male-first-names.txt`
* Add common names from other languages.


### Numbers


```python
# Count how many numbers are in the vocabulary.
count = 0

# For each token in the vocabulary...
for token in tokenizer.vocab.keys():

    # Tally if it's a number.
    if token.isdigit():
        count += 1
        
        # Any numbers >= 10,000?
        if len(token) > 4:
            print(token)

print('Vocab includes {:,} numbers.'.format(count))
```

    Vocab includes 881 numbers.



```python
# Count how many dates between 1600 and 2021 are included.
count = 0 
for i in range(1600, 2021):
    if str(i) in tokenizer.vocab:
        count += 1

print('Vocab includes {:,} of 421 dates from 1600 - 2021'.format(count))
```

    Vocab includes 384 of 421 dates from 1600 - 2021

