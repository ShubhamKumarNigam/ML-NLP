{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture (BERT)\n",
    "\n",
    "(__B__idirectional __E__ncoder __R__epresentations from __T__ransformers)\n",
    "\n",
    "[By Introduction to BERT and Transformer:\n",
    "pre-trained self-attention models to\n",
    "leverage unlabeled corpus data](https://cgi.csc.liv.ac.uk/~hang/ppt/BERT%20and%20Transformer.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses of Pre-trained Language Models\n",
    "Language Model pre-training has been used to improve many NLP tasks:\n",
    "* __ELMo__ (Peters et al., 2018)(__E__mbeddings from __L__anguage __Mo__dels)\n",
    "* __OpenAI GPT__ (Radford et al., 2018)\n",
    "* __ULMFit__ (Howard and Rudder, 2018)\n",
    "\n",
    "### Strategies for applying Pre-trained LM\n",
    "Two __existing strategies__ for applying pre-trained language representations to downstream tasks:\n",
    "* __Featured-based__: include pre-trained representations as addition features (e.g., ELMo)\n",
    "* __Fine-tunning__: introduce task-specific parameters and fine-tune the pre-trained (e.g., OpenAI GPT, ULMFit)\n",
    "\n",
    "### Limitations of Current Techniques\n",
    "* LMs in pre-trained are unidirectional, \n",
    "* they restrict the power of the pre-trained representations\n",
    "    * OpenAI GPT used left-to-right architecture\n",
    "    * ELMo concatenates forward and backward language models\n",
    "    \n",
    "### Solution: BERT\n",
    "\n",
    "* __Main Ideas__\n",
    "    * Propose a new pre-training objective so that a deep bidirectional Transformer can be trained\n",
    "        * The __Masked Language Model (MLM)__: The objective is to predict the original word of a masked word based only on its context.\n",
    "        * __Next Sentence Prediction__\n",
    "        \n",
    "* __Merits of BERT__\n",
    "    * Just fine-tune BERT model for specific tasks to achieve SOTA performance\n",
    "    * BERT advances the SOTA for __Eleven (11)__ NLP tasks\n",
    "    \n",
    "### Model Architecture\n",
    "\n",
    "* BERT's model architecture is a __multi-layer Bi-directional Transformer Encoder__\n",
    "    * Transformer architecture was introduced as a novel pure attention-only sequence-to-sequence architecture by Vaswani et al.[Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf).\n",
    "    \n",
    "* Two models with different sizes were investigated\n",
    "    * $BERT_{BASE}$: L = 12, H = 768, A = 12, Total Parameters = 110M\n",
    "    * $BERT_{LARGE}$: L = 24, H = 1024, A = 16, Total Parameters = 340M\n",
    "        * L: No. of layers (Transformer blocks),\n",
    "        * H: Hidden size,\n",
    "        * A: No. of self-attention heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences in Pre-training Model Architectures: BERT, OpenAI GPT, ELMo\n",
    "\n",
    "`Pre-Trained Rep`\n",
    "* Context-Free\n",
    "    * Word2Vec, GloVe\n",
    "* Contextual\n",
    "    * Uni-directional\n",
    "        * OpenAI GPT [Improving Language Understanding\n",
    "by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "    * Bi-directional\n",
    "        * ELMo (Shallow) [Deep contextualized word representations](https://arxiv.org/abs/1802.05365)\n",
    "        * __BERT(Deep)__\n",
    "\n",
    "<img src = \"/images/pretrained_rep.png\" width = \"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "* BERT is a NLP model __developed by Google__ for __pre-training language representations__. \n",
    "* It leverages an enormous amount of plain text data publicly available on the web and is __trained in an unsupervised manner__. \n",
    "* Pre-training a BERT model is a fairly expensive yet one-time procedure for each language.\n",
    "* The goal is to represent a __variable length sentence__ into a __fixed length vector__.\n",
    "\n",
    "## History\n",
    "\n",
    "Transformer architecture was introduced as a novel pure attention-only sequence-to-sequence architecture by Vaswani et al.[Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf). Its ability for parallelizable training and its general performance improvement made it a popular option among NLP (and recently CV) researchers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "\n",
    "[By Jacob Devlin Google AI Language](https://nlp.stanford.edu/seminar/details/jdevlin.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training in NLP\n",
    "* Word embeddings are the basis of deep learning for NLP\n",
    "\n",
    "king --> [-0.5, -0.9, 1.4, …]\n",
    "\n",
    "queen --> [-0.6, -0.8, -0.2, …]\n",
    "* Word embeddings `(word2vec, GloVe)` are often pre-trained on text corpus from co-occurrence statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://cdn-images-1.medium.com/freeze/max/1000/1*0K0LqqhZfdw9qD7i3a2w_A.png?q=20\" width = \"50%\">\n",
    "\n",
    "<img src = \"https://cdn-images-1.medium.com/freeze/max/1000/1*-xI8ZseTPcLFJqga0EoISg.png?q=20\" width = \"80%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://iitk-my.sharepoint.com/:i:/g/personal/sknigam_iitk_ac_in/EXmlfsmp0apDqLVYIL0Nv98Bxcui5aCUTNAVTa9zTh6DOQ?e=5AmdLY\" width = \"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layers in BERT\n",
    "\n",
    "by [How the Embedding Layers in BERT Were Implemented](https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a)\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1238/1*iJqlhZz-g6ZQJ53-rE9VvA.png\" width = \"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embeddings\n",
    "\n",
    "The role of the Token Embeddings layer is to transform words into vector representations of fixed dimension. In the case of BERT, each word is represented as a 768-dimensional vector.\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1400/1*BXjLEnX89ftmFO_l91sP-A.png\" width = \"60%\">\n",
    "\n",
    "The input text is first tokenized before it gets passed to the Token Embeddings layer. Additionally, extra tokens are added at the start `([CLS])` and end `([SEP])` of the tokenized sentence. The purpose of these tokens are to serve as an input representation for classification tasks and to separate a pair of input texts respectively.\n",
    "\n",
    "The tokenization is done using a method called __WordPiece tokenization__. This is a data-driven tokenization method that aims to achieve a balance between vocabulary size and out-of-vocab words. This is way “strawberries” has been split into “straw” and “berries”. \n",
    "\n",
    ">`The Token Embeddings layer will convert each wordpiece token into a 768-dimensional vector representation.` \n",
    "\n",
    "This results in our 6 input tokens being converted into a matrix of shape (6, 768) or a tensor of shape (1, 6, 768) if we include the batch axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment Embeddings\n",
    "\n",
    "Classifying whether two pieces of text are semantically similar. The pair of input text are simply concatenated and fed into the model. So how does BERT distinguishes the inputs in a given pair? The answer is Segment Embeddings.\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1274/1*7p1kuvoafkCziIWYZSG94w.png\" width = \"60%\">\n",
    "\n",
    "The Segment Embeddings layer only has 2 vector representations. The first vector (index 0) is assigned to all tokens that belong to input 1 while the last vector (index 1) is assigned to all tokens that belong to input 2. If an input consists only of one input sentence, then its segment embedding will just be the vector corresponding to index 0 of the Segment Embeddings table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Embeddings\n",
    "\n",
    "Position embeddings will allow BERT to understand that given an input text like:\n",
    "\n",
    "    I think, therefore I am\n",
    "the first “I” should not have the same vector representation as the second “I”.\n",
    "\n",
    "BERT was designed to process input sequences of up to length 512. The authors incorporated the sequential nature of the input sequences by having BERT learn a vector representation for each position. This means that the Position Embeddings layer is a lookup table of size (512, 768) where the first row is the vector representation of any word in the first position, the second row is the vector representation of any word in the second position, etc. \n",
    "\n",
    "Therefore, if we have an input like `“Hello world”` and `“Hi there”`, both `“Hello”` and `“Hi”` will have identical position embeddings since they are the first word in the input sequence. Similarly, both `“world”` and `“there”` will have the same position embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Position Encoding\n",
    "\n",
    "-by [Amirhossein Kazemnejad's Blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://d33wubrfki0l68.cloudfront.net/a76be57763d942798f8081b77edf8f078720cd45/bd31a/img/transformer_architecture_positional_encoding/model_arc.jpg\" width = \"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is positional encoding and Why do we need it in the first place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Position and order of words are the essential parts of any language. They define the grammar and thus the actual semantics of a sentence. Recurrent Neural Networks (RNNs) inherently take the order of word into account; They parse a sentence word by wThe first idea that might come to mind is to assign a number to each time-step within the [0, 1] range in which 0 means the first word and 1 is the last time-step. Could you figure out what kind of issues it would cause? One of the problems it will introduce is that you can’t figure out how many words are present within a specific range. In other words, time-step delta doesn’t have consistent meaning across different sentences.ord in a sequential manner. This will integrate the words’ order in the backbone of RNNs.\n",
    "\n",
    "But the Transformer architecture ditched the recurrence mechanism in favor of multi-head self-attention mechanism. Avoiding the RNNs’ method of recurrence will result in massive speed-up in the training time. And theoretically, it can capture longer dependencies in a sentence.\n",
    "\n",
    "As each word in a sentence simultaneously flows through the Transformer’s encoder/decoder stack, The model itself doesn’t have any sense of position/order for each word. Consequently, there’s still the need for a way to incorporate the order of the words into our model.\n",
    "\n",
    "One possible solution to give the model some sense of order is to add a piece of information to each word about its position in the sentence. We call this “piece of information”, the positional encoding.\n",
    "\n",
    "The first idea that might come to mind is to assign a number to each time-step within the [0, 1] range in which 0 means the first word and 1 is the last time-step. Could you figure out what kind of issues it would cause? One of the problems it will introduce is that you can’t figure out how many words are present within a specific range. In other words, time-step delta doesn’t have consistent meaning across different sentences.\n",
    "\n",
    "Another idea is to assign a number to each time-step linearly. That is, the first word is given “1”, the second word is given “2”, and so on. The problem with this approach is that not only the values could get quite large, but also our model can face sentences longer than the ones in training. In addition, our model may not see any sample with one specific length which would hurt generalization of our model.\n",
    "\n",
    "Ideally, the following criteria should be satisfied:\n",
    "\n",
    "* It should output a unique encoding for each time-step (word’s position in a sentence)\n",
    "* Distance between any two time-steps should be consistent across sentences with different lengths.\n",
    "* Our model should generalize to longer sentences without any efforts. Its values should be bounded.\n",
    "* It must be deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed method\n",
    "\n",
    "The encoding proposed by the authors is a simple yet genius technique which satisfies all of those criteria. First of all, it isn’t a single number. Instead, it’s a `d` dimensional vector that contains information about a specific position in a sentence. And secondly, this encoding is not integrated into the model itself. Instead, this vector is used to equip each word with information about its position in a sentence. In other words, we enhance the model’s input to inject the order of words.\n",
    "\n",
    "Imagine the positional embedding $ \\vec{p_t}$ as a vector containing pairs of sines and cosines for each frequency.\n",
    "In even position they use `sin` and in odd position they use `cos` funtion.\n",
    "\n",
    "\\begin{align}\n",
    "  \\vec{p_t}^{(i)} = f(t)^{(i)} & := \n",
    "  \\begin{cases}\n",
    "      \\sin({\\omega_k} . t),  & \\text{if}\\  i = 2k \\\\\n",
    "      \\cos({\\omega_k} . t),  & \\text{if}\\  i = 2k + 1\n",
    "  \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\text{where,   }\n",
    "\\omega_k = \\frac{1}{10000^{2k / d}}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{p_t} = \n",
    "\\begin{bmatrix} \n",
    "\\sin({\\omega_1}.t)\\\\ \n",
    "\\cos({\\omega_1}.t)\\\\ \n",
    "\\\\\n",
    "\\sin({\\omega_2}.t)\\\\ \n",
    "\\cos({\\omega_2}.t)\\\\ \n",
    "\\\\\n",
    "\\vdots\\\\ \n",
    "\\\\\n",
    "\\sin({\\omega_{d/2}}.t)\\\\ \n",
    "\\cos({\\omega_{d/2}}.t) \n",
    "\\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The intuition\n",
    "\n",
    "### How this combination of sines and cosines could ever represent a position/order?\n",
    "\n",
    "#### Suppose you want to represent a number in binary format, how will that be?\n",
    "\n",
    "\\begin{align}\n",
    "  0: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  8: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  1: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  9: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\ \n",
    "  2: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  10: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\ \n",
    "  3: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  11: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\ \n",
    "  4: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  12: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  5: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  13: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n",
    "  6: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  14: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  7: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  15: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "You can spot the rate of change between different bits. The LSB bit is alternating on every number, the second-lowest bit is rotating on every two numbers, and so on.\n",
    "\n",
    "But using binary values would be a waste of space in the world of floats. So instead, we can use their float continous counterparts - Sinusoidal functions. Indeed, they are the equivalent to alternating bits. Moreover, By decreasing their frequencies, we can go from red bits to orange ones.\n",
    "\n",
    "<img src = \"https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png\" width = \"60%\">\n",
    "\n",
    "\n",
    "__The 128-dimensional positonal encoding for a sentence with the maximum lenght of 50. Each row represents the embedding vector $ \\vec{p_t}$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How word and positional embedding added?\n",
    "\n",
    "for every word $w_t$ in a sentence $[w_1,...w_n]$, Calculating the correspondent embedding\n",
    "\n",
    "\\begin{align}\n",
    "\\psi^\\prime(w_t) = \\psi(w_t) + \\vec{p_t}\n",
    "\\end{align}\n",
    "\n",
    "To make this summation possible, we keep the positional embedding’s dimension equal to the word embeddings’ dimension i.e.\n",
    "\n",
    "$ d_\\text{word embedding} = d_\\text{postional embedding}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Positioning\n",
    "\n",
    "Another characteristic of sinusoidal positional encoding is that it allows the model to attend relative positions effortlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why positional embeddings are summed with word embeddings instead of concatenation?\n",
    "\n",
    "I couldn’t find any theoretical reason for this question. Since summation (in contrast to concatenation) saves the model’s parameters, it is reasonable to reform the initial question to “Does adding the positional embeddings to words have any disadvantages?”. I would say, not necessarily!\n",
    "\n",
    "We will find out that only the first few dimensions of the whole embedding are used to store the information about the positions.\n",
    "\n",
    "And since the embeddings in the Transfomer are trained from scratch, the parameters are probably set in a way that the semantic of words does not get stored in the first few dimensions to avoid interfering with the positional encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doesn't the position information get vanished once it reaches the upper layers?\n",
    "\n",
    "Fortunately, the Transformer architecture is equipped with __residual connections__. Therefore the information from the input of the model (which contains positional embeddings) can efficiently propagate to further layers where the more complex interactions are handled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why are both sine and cosine used?\n",
    "\n",
    "I think, only by using both sine and cosine, we can express the sine(x+k) and cosine(x+k) as a linear transformation of sin(x) and cos(x). It seems that you can’t do the same thing with the single sine or cosine. If you can find a linear transformation for a single sine/cosine, then you don't need both and please let me inform regarding that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Representations\n",
    "\n",
    "We have seen that a tokenized input sequence of length n will have three distinct representations, namely:\n",
    "\n",
    "* Token Embeddings with shape (1, n, 768) which are just vector representations of words\n",
    "* Segment Embeddings with shape (1, n, 768) which are vector representations to help BERT distinguish between paired input sequences.\n",
    "* Position Embeddings with shape (1, n, 768) to let BERT know that the inputs its being fed with have a temporal property.\n",
    "\n",
    "These representations are summed element-wise to produce a single representation with shape (1, n, 768). This is the input representation that is passed to BERT’s Encoder layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for BERT: GLUE\n",
    "\n",
    "__G__eneral __L__anguage __U__nderstanding __E__valuation (GLUE) benchmark: Standard split of data to\n",
    "train, validation, test, where labels for the test set is only held in the server.\n",
    "\n",
    "* __Sentence pair tasks__\n",
    "    * __MNLI__, Multi-Genre Natural Language Inference\n",
    "    * __QQP__, Quora Question Pairs\n",
    "    * __QNLI__, Question Natural Language Inference\n",
    "    * __STS-B__, The Semantic Textual Similarity Benchmark\n",
    "    * __MRPC__, Microsoft Research Paraphrase Corpus\n",
    "    * __RTE__, Recognizing Textual Entailment\n",
    "    * __WNLI__ Winograd NLI is a small natural language inference dataset\n",
    "* __Single sentence classification__\n",
    "    * __SST-2__, The Stanford Sentiment Treebank\n",
    "    * __CoLA__, The Corpus of Linguistic Acceptability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of Attention\n",
    "\n",
    "[by Attention is Overrated](https://mc.ai/attention-is-overrated/)\n",
    "\n",
    "The BERT paper showed us that attention isn’t all you need to achieve good results in NLP, because to achieve their SOTA results they leveraged bidirectional LSTMs in addition to the self attention used in the transformer.\n",
    "But later work has shown not only do we need more than attention, we may not need attention at all. First we can review some limitations of attention:\n",
    "\n",
    "## Quadratic Complexity\n",
    "* Attention has a quadratic complexity in input length, meaning __attention doesn’t scale well over long distances__.\n",
    "\n",
    "* This is because for each position (there are $n$), \n",
    "* we need to attend to every other position in the input (there are also $n$), \n",
    "* for a total of $d$ dimensions in each location, \n",
    "* so in total there are $n²d$ operations needed, where d is the dimension of the input.\n",
    "\n",
    "<img src = \"https://cdn-images-1.medium.com/freeze/max/1000/1*-xI8ZseTPcLFJqga0EoISg.png?q=20\" width = \"80%\">\n",
    "\n",
    "\n",
    "\n",
    "## Limitations of Attention for Long Range Dependencies\n",
    "\n",
    "* People had speculated that the reason attention was so useful was because it could more easily model long range dependencies in the input, but a recent paper [Why Self-Attention?\n",
    "A Targeted Evaluation of Neural Machine Translation Architectures](https://arxiv.org/pdf/1808.08946.pdf) has shown this to be incorrect.\n",
    "\n",
    "* By looking at the performance of CNN, RNN and the transformer on on subject-verb agreement task as the distance between the subject and verb is increased, \n",
    "* they showed that self-attention’s performance degrades faster than CNNs or RNNs. \n",
    "* To overcome this limitation in performance for long range dependencies, many self attention heads are needed.\n",
    "\n",
    "<img src = \"https://cdn-images-1.medium.com/freeze/max/1000/1*aTffkFV_eZYJFO_ZauiL_g.png?q=20\" width = \"60%\">\n",
    "\n",
    "\n",
    "## The Alternative: Lightweight, Dynamic Convolutions\n",
    "\n",
    "In [Pay Less Attention with Lightweight and Dynamic Convolutions](https://openreview.net/pdf?id=SkVhlh09tX), they propose an alternative to self in the form of a two modified convolution layers:\n",
    "\n",
    "1. __Lightweight Convolutions__ — Depthwise separable convolutions with weight sharing\n",
    "2. __Dynamic Convolutions__ — An extension of lightweight convolutions with dynamic weights\n",
    "\n",
    "### Lightweight Convolutions\n",
    "\n",
    "Lightweight convolutions leverage an innovation that is commonly used in vision to create more efficient architectures, known as Depthwise Convolutions.\n",
    "\n",
    "Depthwise Convolutions take one channel as input for each convolution, in contrast to the standard convolution, which takes all channels as input . Only taking a subset of channels as input massively reduces the number of parameters, and as was seen in the vision literature still provides good performance.\n",
    "\n",
    "<img src = \"https://cdn-images-1.medium.com/freeze/max/1000/1*4x27PPu2ZOUpvWQg4jG2gQ.png?q=20\" width = \"50%\">\n",
    "\n",
    "As shown in [MobileNet](https://arxiv.org/pdf/1704.04861.pdf), depthwise convolutions apply a single filter for each channel.\n",
    "Weight Sharing across channels is also added to the depthwise convolution to further reduce the number of parameters. They use the same weights across multiple channels, for a total of $H$ independent weights. In the paper they used a value of $H=16$, so instead of learning $1024$ different filters, only $16$ are learned, in the case that there are 1024 channels.\n",
    "\n",
    "Using the Depthwise convolution reduces the parameters required from $d²k$ to $dk$, where $d$ is the number of channels, and $k$ is filter width. The addition of weight sharing further reduces the number from $dk$ to $Hk$.\n",
    "\n",
    "<img src = \"https://cdn-images-1.medium.com/freeze/max/1000/1*zT65t_0UqO-mjiKH07IRuA.png?q=20\" width = \"70%\">\n",
    "\n",
    "###  Dynamic Convolutions\n",
    "\n",
    "Dynamic convolutions are an extension of lightweight convolutions, where at each time step a different convolution kernel is created using a linear function. The weights depend only on the current location, not using global context. As indicated below, the dynamic convolution is crucial to get state-of-the-art performance without using self-attention.\n",
    "\n",
    "### Ablation Study\n",
    "\n",
    "To see exactly where the benefits of this model come from. \n",
    "Compared the lightweight and dynamic convolutions to the Transformer Big model. \n",
    "\n",
    "* Wide convolution kernels are necessary to replace self attention\n",
    "* Weight sharing (H=1024 to H=16) doesn’t hurt performance\n",
    "* Self-attention works even with limited context size\n",
    "* Dynamic convolution helps, but softmax normalization is needed for convergence\n",
    "\n",
    "### Thoughts\n",
    "\n",
    "* It seems an important result here is wider context is important at higher layers, but not important at lower ones. Their dynamic convolution is somewhat similar to self attention, and both still perform well with the limited context size, so long as the context progressively increases.\n",
    "\n",
    "* Current SOTA for adversarial images on imagenet uses self-attention layers. Is is possible that these could be replaced using wider, or possibly dynamic convolutions at higher layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
