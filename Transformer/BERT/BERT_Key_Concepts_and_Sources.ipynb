{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0NwnCs-fTFW"
   },
   "source": [
    "# BERT Key Concepts and Sources\n",
    "__B__idirectional __E__ncoder __R__epresentations from __T__ransformers.\n",
    "\n",
    "Inspired from [Blog by Chris McCormick](http://mccormickml.com/2019/11/11/bert-research-ep-1-key-concepts-and-sources/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-0pWLjroFLo"
   },
   "source": [
    "## Sources:\n",
    "\n",
    "Paper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "\n",
    "Github Repo: [github.com/google-research/bert](https://github.com/google-research/bert)\n",
    "\n",
    "Google Blog Post: [Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\n",
    "\n",
    "Paper: [Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\n",
    "\n",
    "The Annotated Transformer (Blog Post) [Harvard NLP](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "\n",
    "__Jay Alammar’s Posts:__\n",
    "\n",
    "BERT —— [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)\n",
    "\n",
    "Transformer —— [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "Attention —— [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
    "\n",
    "Coursera Videos(by Andrew Ng’s Sequence Models): [Sequence Models Course on Coursera](https://www.coursera.org/learn/nlp-sequence-models/home/)\n",
    "Course covers:\n",
    "* RNNs\n",
    "* Encoder-Decoder\n",
    "* LSTMs\n",
    "* Bidirectional RNN\n",
    "* Attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUpvfmKfgcm"
   },
   "source": [
    "## Significance\n",
    "* Published October 2018\n",
    "* Impresive benchmark performance\n",
    "* __Transfer Learning__\n",
    "  * BERT is huge and expensive to train\n",
    "  * Leverage pre-training instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MALl6lUMfgpC"
   },
   "source": [
    "BERT works in two steps, \n",
    "1. It uses a large amount of __unlabeled data__ to learn a language representation in an unsupervised fashion called __pre-training__. \n",
    "2. Then, the pre-trained model can be __fine-tuned__ in a supervised fashion using a small amount of __labeled trained__ data to perform various supervised tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWRzG5exfgvt"
   },
   "source": [
    "### Difference between BERT and other Encoder-Decoder models:\n",
    "The general transformer uses an encoder and a decoder network, however, as BERT is a pre-training model, it only uses the encoder to learn a latent representation of the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x8kqjdXpfgsu"
   },
   "source": [
    "__BERT’s state-of-the-art performance is based on two things.__\n",
    "1. Novel pre-training tasks called __Masked Language Model(MLM)__ and __Next Sentense Prediction (NSP)__.\n",
    "2. A lot of data and compute power to train BERT.\n",
    "\n",
    "MLM makes it possible to perform bidirectional learning from the text.\n",
    "\n",
    "However, __Generative Pre-training (GPT)__ used left-to-right training and __ELMo__ used shallow bidirectionality.\n",
    "\n",
    "In conclusion:\n",
    "\n",
    "__BERT is deeply bidirectional, OpenAI GPT is unidirectional, and ELMo is shallowly bidirectional.__\n",
    "\n",
    "### How different from other Pre-trained models:\n",
    "* Pre-trained representations can either be context-free or contextual, and contextual representations can further be unidirectional or bidirectional. \n",
    "\n",
    "* Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary. \n",
    "\n",
    "* For example, the word “bank” would have the same context-free representation in “bank account” and “bank of the river.” \n",
    "\n",
    "* Contextual models instead generate a representation of each word that is based on the other words in the sentence. \n",
    "\n",
    "* For example, in the sentence “I accessed the bank account,” a unidirectional contextual model would represent “bank” based on “I accessed the” but not “account.” \n",
    "\n",
    "* However, BERT represents “bank” using both its previous and next context — “I accessed the ... account”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FEgdrkYWoFe9"
   },
   "source": [
    "### XLNet\n",
    "* XLNet introduces permutation language modeling, where all tokens are predicted but in random order. \n",
    "* This is in contrast to BERT’s masked language model where only the masked (15%) tokens are predicted. \n",
    "* This is also in contrast to the traditional language models, where all tokens were predicted in sequential order instead of random order. \n",
    "* This helps the model to learn bidirectional relationships and therefore better handles dependencies and relations between words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3Lzvj2soFbw"
   },
   "source": [
    "### RoBERTa:\n",
    "* Introduced at Facebook, Robustly optimized BERT approach RoBERTa\n",
    "* It is a retraining of BERT with improved training methodology, 1000% more data and compute power.\n",
    "* To improve the training procedure, RoBERTa removes the Next Sentence Prediction (NSP) task from BERT’s pre-training and introduces dynamic masking so that the masked token changes during the training epochs. \n",
    "* Larger batch-training sizes were also found to be more useful in the training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbmBkg1vGHJd"
   },
   "source": [
    "### DistilBERT:\n",
    "* It learns a distilled (approximate) version of BERT, retaining 97% performance but using only half the number of parameters.\n",
    "* It does not has token-type embeddings, pooler and retains only half of the layers from Google’s BERT.\n",
    "* This is in some sense similar to posterior approximation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Key Concepts and Sources.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
