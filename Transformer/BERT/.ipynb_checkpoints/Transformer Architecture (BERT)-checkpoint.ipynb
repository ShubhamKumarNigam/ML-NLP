{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture (BERT)\n",
    "\n",
    "(__B__idirectional __E__ncoder __R__epresentations from __T__ransformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses of Pre-trained Language Models\n",
    "Language Model pre-training has been used to improve many NLP tasks:\n",
    "* ELMo (Peters et al., 2018)\n",
    "* OpenAI GPT (Radford et al., 2018)\n",
    "* ULMFit (Howard and Rudder, 2018)\n",
    "\n",
    "### Strategies for applying Pre-trained LM\n",
    "Two __existing strategies__ for applying pre-trained language representations to downstream tasks:\n",
    "* __Featured-based__: include pre-trained representations as addition features (e.g., ELMo)\n",
    "* __Fine-tunning__: introduce task-specific parameters and fine-tune the pre-trained (e.g., OpenAI GPT, ULMFit)\n",
    "\n",
    "### Limitations of Current Techniques\n",
    "* LMs in pre-trained are unidirectional, \n",
    "* they restrict the power of the pre-trained representations\n",
    "    * OpenAI GPT used left-to-right architecture\n",
    "    * ELMo concatenates forward and backward language models\n",
    "    \n",
    "### Solution: BERT\n",
    "\n",
    "* __Main Ideas__\n",
    "    * Propose a new pre-training objective so that a deep bidirectional Transformer can be trained\n",
    "        * The __Masked Language Model (MLM)__: The objective is to predict the original word of a masked word based only on its context.\n",
    "        * __Next Sentence Prediction__\n",
    "        \n",
    "* __Merits of BERT__\n",
    "    * Just fine-tune BERT model for specific tasks to achieve SOTA performance\n",
    "    * BERT advances the SOTA for __Eleven (11)__ NLP tasks\n",
    "    \n",
    "### Model Architecture\n",
    "\n",
    "* BERT's model architecture is a __multi-layer Bi-directional Transformer Encoder__\n",
    "    * Transformer architecture was introduced as a novel pure attention-only sequence-to-sequence architecture by Vaswani et al.[Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf).\n",
    "    \n",
    "* Two models with different sizes were investigated\n",
    "    * $BERT_{BASE}$: L = 12, H = 768, A = 12, Total Parameters = 110M\n",
    "    * $BERT_{LARGE}$: L = 24, H = 1024, A = 16, Total Parameters = 340M\n",
    "        * L: No. of layers (Transformer blocks),\n",
    "        * H: Hidden size,\n",
    "        * A: No. of self-attention heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences in Pre-training Model Architectures: BERT, OpenAI GPT, ELMo\n",
    "<!-- http://localhost:8888/view/Documents/GitHub/ML-NLP/images/pretrained_rep.png -->\n",
    "<img src = \"/images/pretrained_rep.png\" width = \"100%\">\n",
    "\n",
    "<!-- <img src=\"/images/Legal_text_ex2.png\" width=\"90%\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "* BERT is a NLP model __developed by Google__ for __pre-training language representations__. \n",
    "* It leverages an enormous amount of plain text data publicly available on the web and is __trained in an unsupervised manner__. \n",
    "* Pre-training a BERT model is a fairly expensive yet one-time procedure for each language.\n",
    "* The goal is to represent a __variable length sentence__ into a __fixed length vector__.\n",
    "\n",
    "## History\n",
    "\n",
    "Transformer architecture was introduced as a novel pure attention-only sequence-to-sequence architecture by Vaswani et al.[Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf). Its ability for parallelizable training and its general performance improvement made it a popular option among NLP (and recently CV) researchers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "\n",
    "[By Jacob Devlin Google AI Language](https://nlp.stanford.edu/seminar/details/jdevlin.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training in NLP\n",
    "* Word embeddings are the basis of deep learning for NLP\n",
    "\n",
    "king --> [-0.5, -0.9, 1.4, …]\n",
    "\n",
    "queen --> [-0.6, -0.8, -0.2, …]\n",
    "* Word embeddings `(word2vec, GloVe)` are often pre-trained on text corpus from co-occurrence statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layers in BERT\n",
    "\n",
    "by [How the Embedding Layers in BERT Were Implemented](https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a)\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1238/1*iJqlhZz-g6ZQJ53-rE9VvA.png\" width = \"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embeddings\n",
    "\n",
    "The role of the Token Embeddings layer is to transform words into vector representations of fixed dimension. In the case of BERT, each word is represented as a 768-dimensional vector.\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1400/1*BXjLEnX89ftmFO_l91sP-A.png\" width = \"60%\">\n",
    "\n",
    "The input text is first tokenized before it gets passed to the Token Embeddings layer. Additionally, extra tokens are added at the start `([CLS])` and end `([SEP])` of the tokenized sentence. The purpose of these tokens are to serve as an input representation for classification tasks and to separate a pair of input texts respectively.\n",
    "\n",
    "The tokenization is done using a method called __WordPiece tokenization__. This is a data-driven tokenization method that aims to achieve a balance between vocabulary size and out-of-vocab words. This is way “strawberries” has been split into “straw” and “berries”. \n",
    "\n",
    ">`The Token Embeddings layer will convert each wordpiece token into a 768-dimensional vector representation.` \n",
    "\n",
    "This results in our 6 input tokens being converted into a matrix of shape (6, 768) or a tensor of shape (1, 6, 768) if we include the batch axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment Embeddings\n",
    "\n",
    "Classifying whether two pieces of text are semantically similar. The pair of input text are simply concatenated and fed into the model. So how does BERT distinguishes the inputs in a given pair? The answer is Segment Embeddings.\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1274/1*7p1kuvoafkCziIWYZSG94w.png\" width = \"60%\">\n",
    "\n",
    "The Segment Embeddings layer only has 2 vector representations. The first vector (index 0) is assigned to all tokens that belong to input 1 while the last vector (index 1) is assigned to all tokens that belong to input 2. If an input consists only of one input sentence, then its segment embedding will just be the vector corresponding to index 0 of the Segment Embeddings table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Embeddings\n",
    "\n",
    "Position embeddings will allow BERT to understand that given an input text like:\n",
    "\n",
    "    I think, therefore I am\n",
    "the first “I” should not have the same vector representation as the second “I”.\n",
    "\n",
    "BERT was designed to process input sequences of up to length 512. The authors incorporated the sequential nature of the input sequences by having BERT learn a vector representation for each position. This means that the Position Embeddings layer is a lookup table of size (512, 768) where the first row is the vector representation of any word in the first position, the second row is the vector representation of any word in the second position, etc. \n",
    "\n",
    "Therefore, if we have an input like `“Hello world”` and `“Hi there”`, both `“Hello”` and `“Hi”` will have identical position embeddings since they are the first word in the input sequence. Similarly, both `“world”` and `“there”` will have the same position embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Position Encoding\n",
    "\n",
    "-by [Amirhossein Kazemnejad's Blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://d33wubrfki0l68.cloudfront.net/a76be57763d942798f8081b77edf8f078720cd45/bd31a/img/transformer_architecture_positional_encoding/model_arc.jpg\" width = \"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is positional encoding and Why do we need it in the first place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Position and order of words are the essential parts of any language. They define the grammar and thus the actual semantics of a sentence. Recurrent Neural Networks (RNNs) inherently take the order of word into account; They parse a sentence word by wThe first idea that might come to mind is to assign a number to each time-step within the [0, 1] range in which 0 means the first word and 1 is the last time-step. Could you figure out what kind of issues it would cause? One of the problems it will introduce is that you can’t figure out how many words are present within a specific range. In other words, time-step delta doesn’t have consistent meaning across different sentences.ord in a sequential manner. This will integrate the words’ order in the backbone of RNNs.\n",
    "\n",
    "But the Transformer architecture ditched the recurrence mechanism in favor of multi-head self-attention mechanism. Avoiding the RNNs’ method of recurrence will result in massive speed-up in the training time. And theoretically, it can capture longer dependencies in a sentence.\n",
    "\n",
    "As each word in a sentence simultaneously flows through the Transformer’s encoder/decoder stack, The model itself doesn’t have any sense of position/order for each word. Consequently, there’s still the need for a way to incorporate the order of the words into our model.\n",
    "\n",
    "One possible solution to give the model some sense of order is to add a piece of information to each word about its position in the sentence. We call this “piece of information”, the positional encoding.\n",
    "\n",
    "The first idea that might come to mind is to assign a number to each time-step within the [0, 1] range in which 0 means the first word and 1 is the last time-step. Could you figure out what kind of issues it would cause? One of the problems it will introduce is that you can’t figure out how many words are present within a specific range. In other words, time-step delta doesn’t have consistent meaning across different sentences.\n",
    "\n",
    "Another idea is to assign a number to each time-step linearly. That is, the first word is given “1”, the second word is given “2”, and so on. The problem with this approach is that not only the values could get quite large, but also our model can face sentences longer than the ones in training. In addition, our model may not see any sample with one specific length which would hurt generalization of our model.\n",
    "\n",
    "Ideally, the following criteria should be satisfied:\n",
    "\n",
    "* It should output a unique encoding for each time-step (word’s position in a sentence)\n",
    "* Distance between any two time-steps should be consistent across sentences with different lengths.\n",
    "* Our model should generalize to longer sentences without any efforts. Its values should be bounded.\n",
    "* It must be deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed method\n",
    "\n",
    "The encoding proposed by the authors is a simple yet genius technique which satisfies all of those criteria. First of all, it isn’t a single number. Instead, it’s a `d` dimensional vector that contains information about a specific position in a sentence. And secondly, this encoding is not integrated into the model itself. Instead, this vector is used to equip each word with information about its position in a sentence. In other words, we enhance the model’s input to inject the order of words.\n",
    "\n",
    "Imagine the positional embedding $ \\vec{p_t}$ as a vector containing pairs of sines and cosines for each frequency.\n",
    "In even position they use `sin` and in odd position they use `cos` funtion.\n",
    "\n",
    "\\begin{align}\n",
    "  \\vec{p_t}^{(i)} = f(t)^{(i)} & := \n",
    "  \\begin{cases}\n",
    "      \\sin({\\omega_k} . t),  & \\text{if}\\  i = 2k \\\\\n",
    "      \\cos({\\omega_k} . t),  & \\text{if}\\  i = 2k + 1\n",
    "  \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\text{where,   }\n",
    "\\omega_k = \\frac{1}{10000^{2k / d}}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{p_t} = \n",
    "\\begin{bmatrix} \n",
    "\\sin({\\omega_1}.t)\\\\ \n",
    "\\cos({\\omega_1}.t)\\\\ \n",
    "\\\\\n",
    "\\sin({\\omega_2}.t)\\\\ \n",
    "\\cos({\\omega_2}.t)\\\\ \n",
    "\\\\\n",
    "\\vdots\\\\ \n",
    "\\\\\n",
    "\\sin({\\omega_{d/2}}.t)\\\\ \n",
    "\\cos({\\omega_{d/2}}.t) \n",
    "\\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The intuition\n",
    "\n",
    "### How this combination of sines and cosines could ever represent a position/order?\n",
    "\n",
    "#### Suppose you want to represent a number in binary format, how will that be?\n",
    "\n",
    "\\begin{align}\n",
    "  0: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  8: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  1: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  9: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\ \n",
    "  2: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  10: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\ \n",
    "  3: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  11: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\ \n",
    "  4: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  12: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  5: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  13: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n",
    "  6: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  14: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  7: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  15: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "You can spot the rate of change between different bits. The LSB bit is alternating on every number, the second-lowest bit is rotating on every two numbers, and so on.\n",
    "\n",
    "But using binary values would be a waste of space in the world of floats. So instead, we can use their float continous counterparts - Sinusoidal functions. Indeed, they are the equivalent to alternating bits. Moreover, By decreasing their frequencies, we can go from red bits to orange ones.\n",
    "\n",
    "<img src = \"https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png\" width = \"60%\">\n",
    "\n",
    "\n",
    "__The 128-dimensional positonal encoding for a sentence with the maximum lenght of 50. Each row represents the embedding vector $ \\vec{p_t}$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How word and positional embedding added?\n",
    "\n",
    "for every word $w_t$ in a sentence $[w_1,...w_n]$, Calculating the correspondent embedding\n",
    "\n",
    "\\begin{align}\n",
    "\\psi^\\prime(w_t) = \\psi(w_t) + \\vec{p_t}\n",
    "\\end{align}\n",
    "\n",
    "To make this summation possible, we keep the positional embedding’s dimension equal to the word embeddings’ dimension i.e.\n",
    "\n",
    "$ d_\\text{word embedding} = d_\\text{postional embedding}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Positioning\n",
    "\n",
    "Another characteristic of sinusoidal positional encoding is that it allows the model to attend relative positions effortlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why positional embeddings are summed with word embeddings instead of concatenation?\n",
    "\n",
    "I couldn’t find any theoretical reason for this question. Since summation (in contrast to concatenation) saves the model’s parameters, it is reasonable to reform the initial question to “Does adding the positional embeddings to words have any disadvantages?”. I would say, not necessarily!\n",
    "\n",
    "We will find out that only the first few dimensions of the whole embedding are used to store the information about the positions.\n",
    "\n",
    "And since the embeddings in the Transfomer are trained from scratch, the parameters are probably set in a way that the semantic of words does not get stored in the first few dimensions to avoid interfering with the positional encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doesn't the position information get vanished once it reaches the upper layers?\n",
    "\n",
    "Fortunately, the Transformer architecture is equipped with __residual connections__. Therefore the information from the input of the model (which contains positional embeddings) can efficiently propagate to further layers where the more complex interactions are handled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why are both sine and cosine used?\n",
    "\n",
    "I think, only by using both sine and cosine, we can express the sine(x+k) and cosine(x+k) as a linear transformation of sin(x) and cos(x). It seems that you can’t do the same thing with the single sine or cosine. If you can find a linear transformation for a single sine/cosine, then you don't need both and please let me inform regarding that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Representations\n",
    "\n",
    "We have seen that a tokenized input sequence of length n will have three distinct representations, namely:\n",
    "\n",
    "* Token Embeddings with shape (1, n, 768) which are just vector representations of words\n",
    "* Segment Embeddings with shape (1, n, 768) which are vector representations to help BERT distinguish between paired input sequences.\n",
    "* Position Embeddings with shape (1, n, 768) to let BERT know that the inputs its being fed with have a temporal property.\n",
    "\n",
    "These representations are summed element-wise to produce a single representation with shape (1, n, 768). This is the input representation that is passed to BERT’s Encoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
