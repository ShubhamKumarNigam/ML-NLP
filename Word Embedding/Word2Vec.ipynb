{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "by Jaron Collis\n",
    "[Glossary of Deep Learning: Word Embedding](https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://miro.medium.com/max/2000/1*52X2L01wpUjy39lIjofC7g.jpeg\" width = \"80%\">\n",
    "\n",
    "`A plot of word embeddings in English and German. The semantic equivalence of words has been inferred by their context, so similar meanings are co-located. This is because the relative semantics of words are consistent, whatever the language. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What ?\n",
    "\n",
    "Turn text into numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Numbers?\n",
    "\n",
    "This transformation is necessary because many ML algorithms (including deep nets) require their input to be vectors of continuous values; they just won’t work on strings of plain text.\n",
    "\n",
    "So a natural language modelling technique like Word Embedding is used to map words or phrases from a vocabulary to a corresponding vector of real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties\n",
    "\n",
    "* __Dimensionality Reduction__ — it is a more efficient representation.\n",
    "* __Contextual Similarity__ — it is a more expressive representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compared with BoW (Bag of Words)\n",
    "\n",
    "BoW is huge, very sparse __one-hot__ encoded vectors, where the __dimensionality__ of the vectors representing each document is equal to the size of the supported __vocabulary__. \n",
    "\n",
    "Word Embedding aims to create a vector representation with a much lower dimensional space. These are called Word Vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uses\n",
    "\n",
    "Word Vectors are used for __semantic parsing__, to extract meaning from text to enable natural language understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "`Each word has an associated vector, hence the name: word2vec.`\n",
    "\n",
    "Two ways:\n",
    "1. CBOW (Continuous Bag-Of-Words)\n",
    "2. Skip-gram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBoW\n",
    "\n",
    "* In CBOW, we have a __window__ around some __target word__,\n",
    "* Then consider the words around it (its context). \n",
    "* We __supply those words as input__ into our network and then use it to try to __predict the target word__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram\n",
    "\n",
    "Skip-gram does the opposite, you have a __target word__, and you try to predict the words that are in the window around that word, i.e. __predict the context around a word__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How\n",
    "\n",
    "* The __input words__ are passed in as __one-hot__ encoded vectors. \n",
    "* This will go into a __hidden layer__ of linear units, then into a __softmax layer__ to make a prediction. \n",
    "* The idea here is to train the hidden layer weight matrix to find efficient representations for our words. \n",
    "* This weight matrix is usually called the __embedding matrix__, and can be queried as a look-up table.\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1400/0*kx5_UXWs7Q_c071d.\" width = \"70%\">\n",
    "\n",
    "`The word2vec architecture consists of a hidden layer and an output layer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix\n",
    "\n",
    "* The embedding matrix has a __size of the number of words by the number of neurons__ in the hidden layer (the embed size).\n",
    "* So, if you have 10,000 words and 300 hidden units, the matrix will have size 10,000×300 (as we’re using one-hot encoded vectors for our inputs). \n",
    "* Once computed, getting the word vector is a speedy O(1) lookup of corresponding row of the results matrix:\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1242/0*IFv_QtwBNHfGy1Tm.\" width = \"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Similarities between words\n",
    "\n",
    "The classic example is subtracting the ‘notion’ of `“King”` from `“Man”` and adding the notion of `“Woman”` and results being the word `“Queen”`.\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1400/0*1ndbQpbmRrzZWTjO.png\" width = \"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
